{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matthewbelcher/ND-HFTT/blob/main/Assignments/Assignment1/Assignment_1_MLP_Regressor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 1 - MLP Regression\n",
        "\n",
        "# Student Name: Ryan Kennedy\n",
        "\n",
        "The objective of this assignment is to make you familiar with solving **regression** problems with multi-layer perceptrons.\n",
        "\n",
        "[starting point](https://github.com/christianversloot/machine-learning-articles/blob/main/how-to-create-a-neural-network-for-regression-with-pytorch.md)"
      ],
      "metadata": {
        "id": "laSSFhE6jK1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Background\n",
        "\n",
        "Neural nets estimate functions.\n",
        " - some functions have _categorical_ values (i.e., a small set of integers) that represent classes (e.g., diseased/healthy, identity (name), apple/orange/banana/grape, cat/dog). Systems that\n",
        " estimate the category from the samples (observations, measurements, patterns, etc.) are _classifiers_.\n",
        " - some fuctions are continuous (or nearly so) and learning those is called _regression_; systems that perform regression are _regressors_. Examples include invesment yield estimation, positions of vehicles viewed by a drone, age of a tree, etc.\n",
        "The [universal approximation theorem](https://www.machinecurve.com/index.php/2019/07/18/can-neural-networks-approximate-mathematical-functions/)\n",
        "says that any continuous map can be estimated to within a specified an error bound by a sufficiently complex network of two layers.\n",
        "\n",
        "The steps to implement a regressor in Pytorch are very similar to those you'd follow to implement a classifier. However:\n",
        " * The input data has a different _target_ (the known value of the function for a given input, instead of a known class label for a given input).\n",
        " * The loss function will be different.\n",
        " * and, of course, the layers/activations used are problem-specific.\n",
        "\n",
        "But the flow of data (forward and backward) is the same, and training uses the same machinery."
      ],
      "metadata": {
        "id": "VC-RdflHj1Hj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll pose a regression problem; you'll design a MLP to solve it, and you'll run experiments that can help you understand how well it worked.\n",
        "\n",
        "The basic steps below are:\n",
        "1. `import` what we need\n",
        "2. Set up training data\n",
        "3. Design the network\n",
        "4. Configure training process: dataloader for training data, loss, optimizer, hyperparameters like number of epochs and learning rate.\n",
        "5. Implement a training loop and several experiments.\n",
        "6. Assess the results.\n"
      ],
      "metadata": {
        "id": "oEuK5Th8k4vB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1\n",
        "\n",
        "Import all the things you need in the cell below. A starting set of imports is provided for you."
      ],
      "metadata": {
        "id": "AL5gvkolr0o2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STUDENT CODE HERE\n",
        "# import all the things you need.\n",
        "import os\n",
        "import sys\n",
        "import pickle\n",
        "import random\n",
        "import gdown\n",
        "import tqdm.notebook as tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.preprocessing\n",
        "import torch\n",
        "import torchsummary\n"
      ],
      "metadata": {
        "id": "Axk5OY30qb8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2\n",
        "\n",
        "a. Use `torch.random.seed()` to seed the random number generator used by Pytorch in these experiments.\n",
        "\n",
        "b. Determine whether CUDA is available. If so, set `device` to `torch.device('cuda')`; if not, set `device` to `torch.device('cpu')` ."
      ],
      "metadata": {
        "id": "CgpC3KEZsCM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STUDENT CODE GOES HERE\n",
        "#\n",
        "\n"
      ],
      "metadata": {
        "id": "YwXvotDXsBZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3 - Create a function\n",
        "\n",
        "Here, you're going to create your own personal polynomial-transcendental function, depict it, and get some noisy samples from it.\n",
        "\n",
        "In the code cell below, do these things\n",
        "1. Run `np.random.seed(sid)` where `sid` is your NDID number from your ID card student ID number\n",
        "1. Use `np.random.randint()` to generate three random integers between 1 and 7, inclusive, sorting them in ascending order and placing the results in a numpy array named `rands`\n",
        "1. Define a function `f(x,c=rands)` which computes the scalar function $f(x) = c_0\\cdot x^3 - c_1\\cdot x^2 + 2\\cdot c_2^2 \\cdot \\sin(c_3\\cdot x)$ . The subscripts on the $c_i$ are the same as the array indices on the `c[i]`.\n",
        "\n",
        "1. Create `N=512` noisy observations of $f(x)$ on the domain [-5,5] as follows:\n",
        "  1. Generate a vector `X` containing `N` uniformly distributed random numbers between -5 and 5. Use `np.random.uniform()` to generate these numbers. Let the $i^{th}$ element of `X` be denoted $x_i$.\n",
        "  1. Using `np.random.normal()`, generate a vector `Z` of `N` samples from a normal (Gaussian) distribution with mean (location) 0 and standard deviation (scale) 10. Let the $i^{th}$ element of `Z` be denoted $z_i$.\n",
        "  2. Generate a vector `yo`, where the $i^{th}$ element $yo_i = f(x_i) + z_i$.\n",
        "1. Plot the function $f(x)$ on the domain [-5, 5] as a black line and plot the samples $(x_i,yo_i)$ as blue dots on the same plot.  Hints:\n",
        " + `np.linspace()` can give you a set of equally-spaced $x$ values that you can feed to $f()$ to get the corresponding $y$ values for the black line plot.\n",
        " + If you use matplotlib, `plt.plot()` is the sensible way to do the line plot, followed by `plt.scatter()` for the dots, followed by `plt.slabel()` and `plt.ylabel()` for axis labels \"$x$\" and \"$f(x)$\", followed by `plt.show()` to render the whole thing."
      ],
      "metadata": {
        "id": "dttwdVlxwZla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STUDENT CODE GOES HERE\n",
        "np.random.seed( ... )\n",
        "\n",
        "rands = sorted(list(np.random.randint( ... )))\n",
        "\n",
        "def f(x,c=rands):\n",
        "    return ...\n",
        "\n",
        "# sample the function, noisily, at random places in the domain [-5,5]\n",
        "nSamples = ...\n",
        "X = np.random.uniform( ... )\n",
        "Z = np.random.normal( ... )\n",
        "yo = ...\n",
        "\n",
        "xplot = np.linspace(start=-5, stop=5, num=1000)\n",
        "yplot = ...\n",
        "\n",
        "# plotting things ...\n",
        "\n"
      ],
      "metadata": {
        "id": "FZhKs06FwY9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 2: data set\n",
        "\n",
        "Write code that implements a subclass of `torch.utils.data.Dataset` named `FuncDataset`, with `__init__(self,X,y)`, `__len__(self)`, and `__getitem__(self,idx)` methods that instantiate an instance with input and output samples, provide its length, and provide the `idx`'th (X,y) tuple from the instance.\n",
        "\n",
        "You should `np.vstack()` `X` and `y` before storing them in instance variables. Torch really wants individual samples and targets to be rows in an array (tensor) and numpy doesn't do that by default.\n",
        "\n",
        "Remember - it's not necessary to call the superclass constructor in the `__init__()` method, although it does not harm anything if you do."
      ],
      "metadata": {
        "id": "osPh_0mC8zt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STUDENT CODE GOES HERE\n",
        "class FuncDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self,X,y):\n",
        "        \"\"\" initialize the object with numpy arrays of X and y data \"\"\"\n",
        "        ...\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        ...\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        ...\n"
      ],
      "metadata": {
        "id": "CuJ8glfN96M-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3: Instantiate data set; create dataloaders\n",
        "\n",
        "Instantiate `FuncDataSet` with `x` and `yo` (calculated in Task 1) as the data and target values.\n",
        "\n",
        "Use `torch.utils.data.random_split()` with `f_ds` and a 75%:25% training/testing split to create two new datasets named `train_data` and `test_data`; wrap these in `torch.utils.data.DataLoader`s with a batch size of 64 and `shuffle=True`."
      ],
      "metadata": {
        "id": "6u9ci1q9_5J7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STUDENT CODE GOES HERE\n",
        "\n",
        "# instantiate data set\n",
        "\n",
        "# define fractions and split the data set\n",
        "\n",
        "# define batch_size and attach dataloaders to the datasets\n"
      ],
      "metadata": {
        "id": "3A7AHRC-AD23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4: Define MLP network\n",
        "\n",
        "Write code that implements a subclass of `torch.nn.Module` named `FMLP`. It has one `Linear` input layer, two `Linear` hidden layers of `nHidden` units each, and a `Linear` output layer of one unit. Thus, it consumes scalar input, does _neural network magic_ with it, and provides a scalar output.  Use `torch.nn.ReLU()` activations.\n",
        "\n",
        "I recommend wrapping the layers in `torch.nn.Sequential` in the constructor because it's cleaner, and makes the `forward()` method short."
      ],
      "metadata": {
        "id": "M-bYXlcCAyyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#STUDENT CODE GOES HERE\n",
        "class FMLP(torch.nn.Module):\n",
        "    \"\"\"function approximation MLP.\"\"\"\n",
        "    def __init__(self,nHidden=10):\n",
        "        \"\"\"constructor.\"\"\"\n",
        "        super(FMLP,self).__init__()\n",
        "        ...\n",
        "\n",
        "    def forward(self,x):\n",
        "        \"\"\"predict f(x).\"\"\"\n",
        "        ..."
      ],
      "metadata": {
        "id": "qntuOlJTB3ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 5: Experiment Function\n",
        "\n",
        "Here, you'll define a function that runs a single experiment with a hyperparameter (the number of hidden layer units) that can be conveniently adjusted. The function returns the training loss, testing loss, and the trained model.\n",
        "\n",
        "Implement a training loop function `fmlptrain(nHidden=1)` that does this:\n",
        "1. instantiate an FMLP with the specified number of hidden units\n",
        "2. choose a loss function (use whichever of `MSELoss()` or `L1Loss()` that you didn't use in the other notebook for this assignment)\n",
        "3. Choose an optimizer (`Adam` or `SGD` are fine choices)\n",
        "4. run a training loop for 1536 (= 1024 + 512) epochs\n",
        "5. Print out the running training loss divided by the length of the training dataloader at the end of every 128th epoch.\n",
        "6. Also print out the training loss at the end of training.\n",
        "7. Calculates the loss on the testing data.\n",
        "7. returns a tuple:`(final training loss/len(train_loader), testing loss/len(test_loader), model.to('cpu'))`\n",
        "\n",
        "Note: It's wise to make sure the trained model is on the CPU, not the GPU at end-of-function time; hence the `.to('cpu')`"
      ],
      "metadata": {
        "id": "Y-tHEffy8Ae2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#STUDENT CODE GOES HERE\n",
        "def fmlptrain(nHidden=1):\n",
        "    # instantiate net\n",
        "\n",
        "    # define loss fn, LR, optimizer, max_epochs\n",
        "\n",
        "    # move net to device, put in training mode\n",
        "\n",
        "\n",
        "    for epoch in tqdm.trange(max_epochs):\n",
        "        # initialize curent loss\n",
        "        # loop over batches in train_loader\n",
        "            # convert X, y to float and move to device\n",
        "            # zero out gradient estimates in the optimizer\n",
        "            # predict\n",
        "            #compute loss\n",
        "            # backpropagate\n",
        "            # step optimizer\n",
        "            # accumulate loss\n",
        "        # if epoch number mod 128 == 0, print current training loss/len(train_loader)\n",
        "    # calculate testing loss (make sure net is in eval mode)\n",
        "    # return training loss/len(train_loader), testing loss/len(test_loader), trained model.to('cpu')\n"
      ],
      "metadata": {
        "id": "ujkUp4M73SYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 6: Experiment Execution\n",
        "\n",
        "Run the `fmlptrain()` function for the following 14 values of `nHidden`: `[1, 2, 3, 4, 6, 8, 12, 16, 24, 32, 48, 64, 96, 128]`. Save the network after you've trained it. Make a plot of the final training loss in green and the test loss in red versus `nHidden` - use a log scale for the `x` axis of the graph to avoid squashing all the small-x values together.\n",
        "\n",
        "To save a trained model, you have to yank it out of the GPU (if it's there) and make sure it's in evaluation mode. If you store the networks in a dict that is keyed by the value of nHidden, this will work: `model_dict[nHidden] = fmlp.to('cpu').eval()`"
      ],
      "metadata": {
        "id": "bODPjwzUT9GY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STUDENT CODE GOES HERE\n",
        "\n",
        "# initialize dicts for losses, models\n",
        "# loop over all values of nHidden in the experiment\n",
        "    # run training, get trainloss, testloss, model\n",
        "    # store losses and model in the dicts\n",
        "\n",
        "# plot training losses versus nHidden in red (use 'training' label; legend will use it)\n",
        "# plot testing lodded versus nHidden in green (use 'testing' label)\n",
        "# set x axis to log scale (plt.xscale())\n",
        "# set axis labels\n",
        "# create a legend\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "F6ONqlyiHN5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 7: Visualize and Comment\n",
        "\n",
        "Write code that renders (as a line plot) the original function $f(x)$, and the approximation learned by each of the 14 models trained above. Each of these line plots must be a different color (use `label=` in `plt.plot()` to do this the easy way). The basic approach is something like:\n",
        " + If necessary, use `linspace()` to generate 500 values of x between -5 and 5. Call that `xplot`\n",
        " + `plt.plot(xplot,f(xplot), ...)`\n",
        " + prep xplot for presentation to trained nets: `X = torch.tensor(np.vstack(xplot)).float()`\n",
        " + foreach value of `nHidden`:\n",
        "   + let `m` be the model trained with `nHidden` hidden units\n",
        "   + `y = m(X).detach().numpy()`\n",
        "   + `plt.plot(xplot,y, ...)`\n",
        " + `plt.legend()`\n",
        " + `plt.show()`\n",
        "\n",
        " Comment in the area immediately below about the quality of the approximation as a function of the number of hidden units. Was this expected?  If you want to disable some plots and rerun the cell to look at a subset of curves with less clutter, please feel free to do so.\n",
        "\n",
        "\n",
        " # Answer:\n",
        "Surprising how few hidden layer units are needed. Different students may have different experiences."
      ],
      "metadata": {
        "id": "enNyFnr7dPOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# STUDENT CODE GOES HERE\n",
        "\n",
        "# create tensor from vstracked x-values from linspace, and .float() the result, call it X\n",
        "# for each value of nHidden\n",
        "    # get the model for nHidden\n",
        "    # present X to the model, get y, .detach().numpy() it, call it yplot\n",
        "    # plot yplot vs x-values from linspace, specify nHidden as label\n",
        "# plot original function values f(x) for linspace x-values, label it 'f(x)'\n",
        "# create a legend\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "GSVJxI-XYmZT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}